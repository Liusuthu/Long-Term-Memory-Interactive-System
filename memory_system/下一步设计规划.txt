预处理环节：
    一拿到Conversation后要先存入container中✅，
    定义好一个统一的LLM接口，方便自由调用✅
    存入后提取key/fact并存入数据结构对应位置，并重置has_keys/facts标记✅
    存储后遍历计算这些key/fact的嵌入向量，可以存储在container的平行位置处✅

解答环节：
    分析具体的问题，planner生成一个list的步骤，让我们的系统step by step地执行✅
    针对某一question，按照不同的策略将其转化为要用的query(暂时使用question本身)✅⭕️
    按照指定好的query，遍历container结构寻找相似度最高的部分（颗粒度：session✅/round/hybrid）
    寻找后返回chunks，输入给reader进行处理操作，最终得到回答✅
    Advanced：使用scheduler看看是否还需要更换query重新检索⭕️
    Judge: Use LLM as Judge ✅
    考虑使用本地的Embedding Model，不知道是否能省钱？但是OpenAI的emb模型花不了多少钱。

评价指标：⭕️
    1️⃣检索命中率
    2️⃣QA正确率（LLM as Judge）


⚠️亟待解决：现在好多模块都单独初始化LLM，一方面显存占用大，另一方面不符合端侧应用场景的要求，另一方面显存浪费占用，可以考虑优化
    一共载入两个模型：14B和3B，14B可以作为Extractor和Reader，3B作为Judge ✅（目前占用35.5G，这样一张卡可以跑两个任务了）
⚠️注：如果要创造一个self_motivated_and_scheduled的系统的话，画好流程图，规划好planner和scheduler的固定输出和预计内容子集很重要。
    这也是后续工作的努力方向。


一些小结论：
    对于larger model，使用7B和14B的差别：14B能让效果好很多，包括提取fact的能力，以及最终总结生成答案的能力；换成7B准确率直线下降。而且指令跟随能力差容易报错。
    如果用32B呢？会不会强悍爆表？——运行太久了！！！！
    使用本地Emb函数与OpenAI的Emb函数哪个更好一些：


初步试验后的下一步计划：
    改进提示词，针对部分问题进行优化处理：引入问题分类路由，随后导入到不同的prompt处理，评估命中率。
    改进Judge的评判机制，优化判决结果✅
    考虑引入planner，分析问题后生成一个可行的解决路径（甚至可以结合小llm微调，体现定制化），重点在于对不同问题采取不同策略✅⭕️


export PYTHONPATH=$PYTHONPATH:/home/limusheng/Long-Term-Memory-Interactive-System